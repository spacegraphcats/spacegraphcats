#
# Snakemake configuration file for running spacegraphcats pipelines.
#
# Quickstart: `spacegraphcats dory-test searchquick`
#
from os.path import join

## pull values in from config file / command line:

# set working directory
startdir = os.getcwd()
outdir = config.get('outdir')
if not outdir:
    outdir = startdir

workdir: outdir

def fix_relative_input_paths(filenames):
    abs_filenames = []
    for filename in filenames:
        if not filename.startswith('/'):
            print('...updating input file {} with prefix {}'.format(filename, startdir))
            filename = os.path.join(startdir, filename)
        abs_filenames.append(filename)
    return abs_filenames

# name of catlas
catlas_base = config['catlas_base']

# sequence files to use when building catlas
input_sequences = fix_relative_input_paths(config['input_sequences'])

# k-mer size to use for everything
ksize = config['ksize']

# radius for catlas building
radius = config['radius']

# search overhead
overhead = config.get('overhead', 0.0)

# fix paths to searchquick, search files
config['search'] = fix_relative_input_paths(config.get('search', []))
config['searchquick'] = fix_relative_input_paths(config.get('searchqiuck', []))

# paper evaluation: do not expand the cDBG nodes using the domset
cdbg_only = ""
catlas_suffix = ""
if config.get('cdbg_only', False):
   cdbg_only = "--cdbg-only"
   catlas_suffix = "_cdbg"

# suffix to add to search output
experiment = config.get('experiment', '')
search_out_suffix = ''
if experiment:
    search_out_suffix = '_' + experiment

# remove pendants from bcalm graph? default YES
remove_pendants = ""
if config.get('keep_graph_pendants', False):
    remove_pendants = "-P"

# minsize/maxsize for decomposition
decompose_minsize = config.get('decompose_minsize', 5000)
decompose_maxsize = config.get('decompose_maxsize', 50000)

# hashval queries / query_by_hashval
hashval_ksize = config.get('hashval_ksize', ksize)

hashval_queries = ''
if config.get('hashval_queries', ''):
    filename = config.get('hashval_queries', '')
    hashval_queries = fix_relative_input_paths([filename])[0]

### build some variables for internal use

catlas_dir = '{}_k{}_r{}{}'.format(catlas_base, ksize, radius, catlas_suffix)
search_dir = '{}_search_oh{}{}'.format(catlas_dir, int(overhead*100), search_out_suffix)
hashval_query_dir = '{}_hashval_k{}'.format(catlas_dir, hashval_ksize)

# internal definitions for convenience:
python=sys.executable  # define as the version of Python running snakemake

onsuccess:
    import os
    print('\n-------- DONE --------\n')
    if os.path.exists(catlas_dir):
        print('catlas output directory: {}'.format(catlas_dir)) # @@
        if os.path.exists(search_dir):
            print('search output directory: {}'.format(search_dir))
        print('')


###############################################################################

### rules!

# build catlas needed for search
rule build:
    input:
        expand("{catlas_dir}/catlas.csv", catlas_dir=catlas_dir),
        expand("{catlas_dir}/contigs.fa.gz.mphf", catlas_dir=catlas_dir),
        expand("{catlas_dir}/contigs.fa.gz.indices", catlas_dir=catlas_dir),
        expand("{catlas_dir}/contigs.fa.gz.info.csv", catlas_dir=catlas_dir),

rule clean:
    shell:
        "rm -fr {catlas_base} {catlas_dir} {search_dir}"

# build cDBG using bcalm
rule bcalm_cdbg:
     input:
        "{catlas_base}/bcalm.{catlas_base}.k{ksize}.inputlist.txt"
     output:
        "{catlas_base}/bcalm.{catlas_base}.k{ksize}.unitigs.fa"
     shell:
        "(bcalm >& /dev/null || (printf '**\\n**\\n** Cannot run BCALM 2! Please install it and make sure it is on your path!\\n**\\n**\\n'; exit 1)) && (bcalm >& /dev/null && bcalm -in {input} -out {catlas_base}/bcalm.{catlas_base}.k{wildcards.ksize} -kmer-size {wildcards.ksize} -abundance-min 1 >& {output}.log.txt && rm -f {catlas_base}/bcalm.{catlas_base}.k{wildcards.ksize}.{{h5,unitigs.fa.glue*}} || printf '\\n**\\n** ERROR running BCALM 2; err output in dory/bcalm.dory.k21.unitigs.fa.log.txt\\n**\\n')"

# create list of input files for bcalm
rule bcalm_cdbg_inpfiles:
     input:
        input_sequences
     output:
        "{catlas_base}/bcalm.{catlas_base}.k{ksize}.inputlist.txt"
     run:
        with open(output[0], 'wt') as fp:
            for name in input_sequences:
                fp.write('{}\n'.format(name))

# build catlas input from bcalm output by reformatting
rule bcalm_catlas_input:
     input:
        expand("{catlas_base}/bcalm.{catlas_base}.k{ksize}.unitigs.fa", ksize=ksize, catlas_base=catlas_base)
     output:
        join(catlas_dir, "cdbg.gxt"),
        join(catlas_dir, "contigs.fa.gz"),
        join(catlas_dir, "contigs.fa.gz.info.csv")
     shell:
        "{python} -m spacegraphcats.cdbg.bcalm_to_gxt {remove_pendants} -k {ksize} {input} {catlas_dir}/cdbg.gxt {catlas_dir}/contigs.fa.gz"

rule reads_bgzf:
     input:
        input_sequences
     output:
        "{catlas_base}/{catlas_base}.reads.bgz"
     shell:
        "{python} -m spacegraphcats.utils.make_bgzf {input} -o {output}"


# label the reads by contig
rule label_reads:
     input:
        expand("{catlas_base}/{catlas_base}.reads.bgz", catlas_base=catlas_base),
        expand("{catlas_dir}/contigs.fa.gz", catlas_dir=catlas_dir)
     output:
        join(catlas_dir, "reads.bgz.labels")
     shell:
        "{python} -m spacegraphcats.cdbg.label_cdbg {catlas_dir} {input[0]} {output}"


# build catlas!
rule build_catlas:
     input:
        join(catlas_dir, "cdbg.gxt"),
        join(catlas_dir, "contigs.fa.gz"),
     output:
        join(catlas_dir, "first_doms.txt"),
        join(catlas_dir, "catlas.csv"),
        join(catlas_dir, "commands.log")
     shell:
        "{python} -m spacegraphcats.catlas.catlas --no_checkpoint {catlas_dir} {radius}"

# index contigs, count node sizes
rule make_contigs_kmer_index:
     input:
        join(catlas_dir, "contigs.fa.gz")
     output:
        join(catlas_dir, "contigs.fa.gz.mphf"),
        join(catlas_dir, "contigs.fa.gz.indices"),
     shell:
        "{python} -m spacegraphcats.index.index_contigs_by_kmer -k {ksize} {catlas_dir}"

### Search rules.

def make_query_base(searchfiles):
    x = []
    if not searchfiles:
        return x
    for filename in searchfiles:
        x.append("{}/{}.contigs.sig".format(search_dir, os.path.basename(filename)))
        x.append("{}/{}.cdbg_ids.txt.gz".format(search_dir, os.path.basename(filename)))
        x.append("{}/{}.frontier.txt.gz".format(search_dir, os.path.basename(filename)))
    return x

# do a quick search!
rule searchquick:
    input:
        config['searchquick'],
        expand("{catlas_dir}/first_doms.txt", catlas_dir=catlas_dir),
        expand("{catlas_dir}/catlas.csv", catlas_dir=catlas_dir),
        expand("{catlas_dir}/contigs.fa.gz.mphf", catlas_dir=catlas_dir),
        expand("{catlas_dir}/contigs.fa.gz.indices", catlas_dir=catlas_dir)
    output:
        expand("{search_dir}/results.csv", search_dir=search_dir),
        make_query_base(config['searchquick']),
    shell:
        "{python} -m spacegraphcats.search.query_by_sequence {catlas_dir} {search_dir} --query {config[searchquick]} -k {ksize} {cdbg_only}"


# do a full search!
rule search:
    input:
        config['search'],
        expand("{catlas_dir}/first_doms.txt", catlas_dir=catlas_dir),
        expand("{catlas_dir}/catlas.csv", catlas_dir=catlas_dir),
        expand("{catlas_dir}/contigs.fa.gz.mphf", catlas_dir=catlas_dir),
        expand("{catlas_dir}/contigs.fa.gz.indices", catlas_dir=catlas_dir)
    output:
        expand("{search_dir}/results.csv", search_dir=search_dir),
        make_query_base(config['search']),
    shell:
        "{python} -m spacegraphcats.search.query_by_sequence {catlas_dir} {search_dir} --query {config[search]} -k {ksize} {cdbg_only}"

ruleorder: search > searchquick

### Extract contigs and reads.

def make_extract_contigs_base(searchfiles):
    x = []
    for filename in searchfiles:
        x.append("{}/{}.cdbg_ids.contigs.fa.gz".format(search_dir, os.path.basename(filename)))
    return x

def make_extract_reads_base(searchfiles):
    x = []
    for filename in searchfiles:
        x.append("{}/{}.cdbg_ids.reads.fa.gz".format(search_dir, os.path.basename(filename)))
    return x

# get contigs for a single query
rule extract_contigs_single_file:
    input:
        expand("{catlas_dir}/contigs.fa.gz", catlas_dir=catlas_dir),
        expand("{search_dir}/{{queryname}}.cdbg_ids.txt.gz", search_dir=search_dir)
    output:
        join(search_dir, "{queryname}.cdbg_ids.contigs.fa.gz")
    shell:
        "{python} -m spacegraphcats.search.extract_contigs {catlas_dir} {input[1]} -o {output}"

# get reads for a single query
rule extract_reads_single_file:
    input:
        expand("{catlas_base}/{catlas_base}.reads.bgz", catlas_base=catlas_base),
        expand("{catlas_dir}/reads.bgz.labels", catlas_dir=catlas_dir),
        expand("{search_dir}/{{queryname}}.cdbg_ids.txt.gz", search_dir=search_dir)
    output:
        join(search_dir, "{queryname}.cdbg_ids.reads.fa.gz")
    shell:
        "{python} -m spacegraphcats.search.extract_reads {input[0]} {input[1]} {input[2]} -o {output}"

# get all the reads
rule extract_reads:
    input:
        make_extract_reads_base(config['search'])

# get all the contigs
rule extract_contigs:
    input:
        make_extract_contigs_base(config['search'])

### catlas decomposition
rule decompose_catlas:
    input:
        expand("{catlas_dir}/catlas.csv", catlas_dir=catlas_dir),
    output:
        directory(expand("{catlas_dir}_decompose", catlas_dir=catlas_dir))
    shell:
        """{python} -m spacegraphcats.search.decompose_catlas {catlas_dir} \
               --minsize={decompose_minsize} --maxsize={decompose_maxsize} \
               {output}"""

rule extract_reads_for_decomposition:
    input:
        expand("{catlas_base}/{catlas_base}.reads.bgz", catlas_base=catlas_base),
        expand("{catlas_dir}/reads.bgz.labels", catlas_dir=catlas_dir),
        expand("{catlas_dir}_decompose", catlas_dir=catlas_dir)
    shell:
        """for i in {input[2]}/*.txt.gz; do
              python -m spacegraphcats.search.extract_reads \
                {input[0]} {input[1]} $i \
                -o {input[2]}/$(basename $i .txt.gz).reads.gz;
           done"""

### hashval query stuff

# build hashval query index
rule build_hashval_query_index:
    input:
        expand("{catlas_dir}/contigs.fa.gz", catlas_dir=catlas_dir)
    output:
        expand("{hashval_query_dir}/index.pickle", hashval_query_dir=hashval_query_dir)
    shell:
        "{python} -m spacegraphcats.cdbg.index_cdbg_by_minhash -k {hashval_ksize} {input[0]} {output}"

# do a full search!
checkpoint hashval_query:
    input:
        hashval_queries,
        expand("{hashval_query_dir}/index.pickle", hashval_query_dir=hashval_query_dir),
        expand("{catlas_dir}/catlas.csv", catlas_dir=catlas_dir)
    output:
        expand("{hashval_query_dir}/hashval_results.csv", hashval_query_dir=hashval_query_dir),
        directory(expand("{hashval_query_dir}/contigs", hashval_query_dir=hashval_query_dir))
#        make_query_base(config['search']),
    shell:'''
        mkdir -p {hashval_query_dir}/contigs
        {python} -m spacegraphcats.search.query_by_hashval \
                 -k {hashval_ksize} {catlas_dir} \
                 {hashval_query_dir}/index.pickle \
                 {hashval_queries} {hashval_query_dir}
    '''

# using output of hashval_query, generate names for output of extract_reads_single_hashval_file
def aggregate_hashval_query(wildcards): 
    checkpoint_output = checkpoints.hashval_query.get(**wildcards).output[1]
    hashval_names = expand("{hashval_query_dir}/reads/{hashval}.cdbg_ids.reads.fa.gz",
                           hashval_query_dir = hashval_query_dir,
                           hashval = glob_wildcards(os.path.join(checkpoint_output, "{hashval}.contigs.fa.gz")).hashval)
    return hashval_names

# get reads for a single hashval query
rule extract_reads_single_hashval_file:
    input:
        expand("{catlas_base}/{catlas_base}.reads.bgz", catlas_base=catlas_base),
        expand("{catlas_dir}/reads.bgz.labels", catlas_dir=catlas_dir),
        expand("{hashval_query_dir}/contigs/{{hashval}}.cdbg_ids.txt.gz", hashval_query_dir=hashval_query_dir)
    output: expand("{hashval_query_dir}/reads/{{hashval}}.cdbg_ids.reads.fa.gz", hashval_query_dir=hashval_query_dir)
    shell:
        "{python} -m spacegraphcats.search.extract_reads {input[0]} {input[1]} {input[2]} -o {output}"

# get reads for all the hashvals
rule extract_reads_for_hashvals:
    input:
        hashval_queries,
        expand("{hashval_query_dir}/hashval_results.csv", hashval_query_dir=hashval_query_dir),
        #make_hashval_cdbg_results(hashval_queries)
        aggregate_hashval_query
        
### shadow ratio stuff

rule shadow_ratio:
    input:
        expand("{catlas_dir}.shadow.{maxsize}.fa",
               catlas_dir=catlas_dir,
               maxsize=config.get('shadow_ratio_maxsize', 1000))

rule extract_by_shadow_ratio_rule:
    input:
        expand("{catlas_dir}/first_doms.txt", catlas_dir=catlas_dir),
        expand("{catlas_dir}/catlas.csv", catlas_dir=catlas_dir),
    output:
        catlas_dir + ".shadow.{shadow_ratio_maxsize}.fa"
    shell:
        "{python} -m spacegraphcats.search.extract_nodes_by_shadow_ratio --maxsize={wildcards.shadow_ratio_maxsize} {catlas_dir} {output}"

### query by signature => gene

rule build_multifasta_hashval_index:
    input:
        expand("{catlas_dir}/contigs.fa.gz", catlas_dir=catlas_dir)
    output:
        expand("{catlas_dir}_multifasta/hashval.pickle", catlas_dir=catlas_dir)
    params:
        ksize = ksize,
        scaled = config.get('multifasta_scaled', 1000)
    shell: """
        {python} -m spacegraphcats.cdbg.index_cdbg_by_minhash \
            -k {params.ksize} --scaled {params.scaled} {input[0]} {output}
    """

rule build_index:
    input:
        queries = config.get('multifasta_reference', '** does not exist **'),
        doms = expand("{catlas_dir}/first_doms.txt", catlas_dir=catlas_dir),
        catlas = expand("{catlas_dir}/catlas.csv", catlas_dir=catlas_dir),
        mphf = expand("{catlas_dir}/contigs.fa.gz.mphf", catlas_dir=catlas_dir),
        indices = expand("{catlas_dir}/contigs.fa.gz.indices", catlas_dir=catlas_dir)
    output:
        expand("{catlas_dir}_multifasta/multifasta.pickle", catlas_dir=catlas_dir)
    shell: """
        {python} -m spacegraphcats.search.index_cdbg_by_multifasta \
             {catlas_dir} {output} --query {input.queries}
    """

rule query_multifasta_by_signature:
    input:
        hashvals = expand("{catlas_dir}_multifasta/hashval.pickle", catlas_dir=catlas_dir),
        multi_idx = expand("{catlas_dir}_multifasta/multifasta.pickle", catlas_dir=catlas_dir),
        query_sig = config.get('multifasta_query_sig', '** does not exist **')
    output:
        expand("{catlas_dir}_multifasta/query-results.csv", catlas_dir=catlas_dir)
    params:
        ksize=ksize,
        scaled = config.get('multifasta_scaled', 1000)
    shell: """
        {python} -m spacegraphcats.search.query_multifasta_by_sig \
            --hashvals {input.hashvals} --multi-idx {input.multi_idx} \
            --query-sig {input.query_sig} --output {output} \
            -k {params.ksize} --scaled {params.scaled}
    """

# this rule also creates a bunch of files named record{n}-nbhd.cdbg_ids.txt.gz
# see rule extract_reads_single_hashval_file for how to get the reads.
rule build_cdbg_list_by_record:
    input:
        expand("{catlas_dir}_multifasta/multifasta.pickle", catlas_dir=catlas_dir)
    output:
        expand("{catlas_dir}_multifasta/multifasta.cdbg_by_record.csv", catlas_dir=catlas_dir)
    shell: """
        {python} -m spacegraphcats.search.extract_cdbg_by_multifasta \
            --multi-idx {input} --output {output}
    """

# get reads for a single multifasta record
rule extract_reads_single_file_multifasta:
    input:
        expand("{catlas_base}/{catlas_base}.reads.bgz", catlas_base=catlas_base),
        expand("{catlas_dir}/reads.bgz.labels", catlas_dir=catlas_dir),
        expand("{catlas_dir}_multifasta/{{queryname}}.cdbg_ids.txt.gz", catlas_dir=catlas_dir),
        expand("{catlas_dir}_multifasta/multifasta.cdbg_by_record.csv", catlas_dir=catlas_dir)
    output:
        expand("{catlas_dir}_multifasta/{{queryname}}.reads.fa.gz", catlas_dir=catlas_dir)
    shell:
        "{python} -m spacegraphcats.search.extract_reads {input[0]} {input[1]} {input[2]} -o {output}"
